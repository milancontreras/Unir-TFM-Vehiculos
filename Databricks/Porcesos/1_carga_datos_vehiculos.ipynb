{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e195f7b-0afb-4ac0-89ea-b2cbf148967f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e6fcc5f-00e9-4d2c-9a37-975b970f1d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Configuraciones/variables_configuraciones\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a28e5f-caf6-4250-bff3-15626c3bcba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(BUCKET_S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e12f6fb-457d-4c32-baa0-ce30f901fbdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import  Window\n",
    "\n",
    "\n",
    "from io import TextIOWrapper, BytesIO\n",
    "import csv\n",
    "import unicodedata\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7eb538-1fd1-45c4-bd8a-82e75a811f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CONFIGURACION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025dbcb6-8683-492a-b23f-dea4acf5cd09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#DESARROLLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cfc7fa1-04ba-4dd7-9111-6a8bca275fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cargar Tabla estado bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccacd2a1-8896-4c75-9ab7-10ca829afc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS externo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4efce5c6-5c37-47b8-b30b-9a1eb6483b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import VolumeType\n",
    "\n",
    "catalog = \"workspace\"\n",
    "schema = \"externo\"\n",
    "volume = \"tfm_s3_datalake_externo\"\n",
    "PATH_VOLUMEN = F\"/Volumes/{catalog}/{schema}/{volume}\"\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Ver si el volume ya existe\n",
    "volumes = [v.name for v in w.volumes.list(catalog_name=catalog, schema_name=schema)]\n",
    "\n",
    "if volume not in volumes:\n",
    "    print(\"Creando volume…\")\n",
    "    w.volumes.create(\n",
    "        name=volume,\n",
    "        catalog_name=catalog,\n",
    "        schema_name=schema,\n",
    "        volume_type=VolumeType.EXTERNAL,\n",
    "        storage_location= BUCKET_S3\n",
    "    )\n",
    "else:\n",
    "    print(\"El volume ya existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874be89a-b014-4967-bcd4-5ec707509423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "path = f\"dbfs:/Volumes/{catalog}/{schema}/{volume}/metadata/vehiculos/state.json\"\n",
    "\n",
    "raw_df = (\n",
    "    spark.read\n",
    "         .option(\"multiline\", \"true\")\n",
    "         .json(path)\n",
    ")\n",
    "\n",
    "years = raw_df.columns\n",
    "expr = \", \".join([f\"'{y}', `{y}`\" for y in years])\n",
    "stack_expr = f\"stack({len(years)}, {expr}) as (year, data)\"\n",
    "\n",
    "sdf_bronce_state = (\n",
    "    raw_df\n",
    "    .selectExpr(stack_expr)  # year, data (struct)\n",
    "    .select(\"year\", \"data.*\")\n",
    ")\n",
    "\n",
    "display(sdf_bronce_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc6b440-3c5c-4381-bbcd-54c6abdf2cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "#from pyspark.sql import functions as F, types as T\n",
    "\n",
    "sdf_bronce_state = spark.createDataFrame(json_data); \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da79e8c7-04c7-491e-9b2a-54a567bfacd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_bronce_state = (sdf_bronce_state\n",
    "  .withColumn(\"year\", F.col(\"year\").cast(\"int\"))\n",
    "  .withColumn(\"last_metadata_update\", F.to_timestamp(\"last_metadata_update\"))\n",
    "  .withColumn(\"last_status\",          F.col(\"last_status\"))\n",
    "  .withColumn(\"last_checked_ts\",      F.to_timestamp(\"last_checked_ts\"))\n",
    "  .withColumn(\"last_file_path\",       F.col(\"last_file_path\"))\n",
    "  .withColumn(\"last_sha256\",          F.col(\"last_sha256\"))\n",
    "  .select(\"year\",\"last_metadata_update\",\"last_status\",\"last_checked_ts\",\"last_file_path\",\"last_sha256\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74397ed4-1243-4633-b55c-a969adb7fcff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_bronce_state.write.mode(\"overwrite\").saveAsTable(\"tfm_bronze.state_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca57433-6dad-4dee-9f27-384631ac77a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"[INFO] tfm_bronze.state_files actualizado desde state.json\")\n",
    "display(spark.table(\"tfm_bronze.state_files\").orderBy(F.col(\"year\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc08733-8a16-4d2a-911a-1673df29e512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verificar Archivos cambiados o nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9599bef6-9d8e-4290-85a6-a99ea2fd1655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import functions as F\n",
    "\n",
    "# Alias para las tablas Bronze (B) y Silver (P)\n",
    "B = spark.table(\"tfm_bronze.state_files\").alias(\"b\")\n",
    "P = spark.table(\"tfm_silver.state_files_processed\").alias(\"p\")\n",
    "\n",
    "# Unimos por año (left join: mantiene todos los registros de Bronze)\n",
    "joined = B.join(P, on=B[\"year\"] == P[\"year\"], how=\"left\")\n",
    "\n",
    "# Aplicamos las condiciones para detectar archivos nuevos o modificados:\n",
    "#   - No existe en Silver\n",
    "#   - El hash SHA256 cambió (contenido diferente)\n",
    "#   - La fecha de metadatos cambió\n",
    "#   - Existe un error registrado en Silver\n",
    "to_process = joined.where(\n",
    "    (P[\"year\"].isNull()) |\n",
    "    (B[\"last_sha256\"] != P[\"last_sha256\"]) |\n",
    "    #(F.to_timestamp(B[\"last_metadata_update\"]) != F.to_timestamp(P[\"last_metadata_update\"])) |\n",
    "    (P[\"error_message\"].isNotNull())\n",
    ").select(\n",
    "    B[\"year\"],\n",
    "    B[\"last_metadata_update\"],\n",
    "    B[\"last_status\"],\n",
    "    B[\"last_checked_ts\"],\n",
    "    B[\"last_file_path\"],\n",
    "    B[\"last_sha256\"],\n",
    "    P[\"last_sha256\"].alias(\"last_sha256_silver\")\n",
    ")\n",
    "\n",
    "# Resultado final: lista de archivos que deben procesarse hacia Silver\n",
    "print(\"[INFO] Archivos NUEVOS o CAMBIADOS que deben procesarse:\")\n",
    "display(to_process.orderBy(\"year\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc1153b-dc57-4c53-9312-0be6fbf310fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b9a5ff1-c77c-45d1-aeee-777d7f9cc7a5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Obtener dataframe a partir de csv dentro de un S3"
    }
   },
   "outputs": [],
   "source": [
    "def obtener_dataframe_csv(key_path: str):\n",
    "    \"\"\"\n",
    "    Crea un dataframe de spark del archivo almacenado en el volumen\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"sep\", \";\") \n",
    "            .option(\"charset\", \"latin1\")\n",
    "            .option(\"inferSchema\", True)\n",
    "            .csv(key_path)\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c855b9-4cf8-4ddd-b82e-fb56a3a74ec4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Normallizar nombre de columnas"
    }
   },
   "outputs": [],
   "source": [
    "def normalizacion_cabecera(df, year: int):\n",
    "    \"\"\"\n",
    "    Normaliza los encabezados de un DataFrame según las reglas definidas \n",
    "    en la tabla de configuración `tfm_config.header_mappings`. \n",
    "    \n",
    "    Esta función asegura que las columnas del DataFrame de entrada \n",
    "    (diferentes entre años o archivos) se ajusten a una nomenclatura \n",
    "    estándar, aplicando las reglas vigentes al momento de ejecución.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        DataFrame de entrada con encabezados originales del archivo CSV.\n",
    "    year : int\n",
    "        Año de la fuente de datos, usado para aplicar las reglas específicas \n",
    "        de ese período en `header_mappings`.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        DataFrame con los encabezados normalizados y alineados \n",
    "        al esquema canónico del dataset “vehiculos”.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # CONFIGURACIÓN BASE Y CONTEXTO TEMPORAL\n",
    "    # ---------------------------------------------------------------------\n",
    "    dataset_id = \"vehiculos\"\n",
    "\n",
    "    # Marca temporal de referencia del proceso (fecha/hora actual)\n",
    "    #event_ts = F.current_timestamp()\n",
    "\n",
    "    # Obtención de un valor literal de timestamp (no columna Spark)\n",
    "    event_ts_lit = spark.range(1).select(F.current_timestamp().alias(\"ts\")).collect()[0][\"ts\"]\n",
    "\n",
    "    #print(\"Columnas Originales:\", df.columns)\n",
    "    # Registro de control para auditoría (puede eliminarse en producción)\n",
    "    #print(event_ts)\n",
    "    #print(event_ts_lit)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # LECTURA DE LA TABLA DE CONFIGURACIÓN DE ENCABEZADOS\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Se carga la tabla de mapeo que define cómo traducir los nombres \n",
    "    # de columnas originales (\"header_source\") hacia los nombres estándar \n",
    "    # o canónicos (\"header_canonical\").\n",
    "    #\n",
    "    # Solo se aplican reglas activas (effective_start <= ahora < effective_end)\n",
    "    # para el dataset \"vehiculos\".\n",
    "    # ---------------------------------------------------------------------\n",
    "    mappings = (\n",
    "        spark.table(\"tfm_config.header_mappings\")\n",
    "        .where(\n",
    "            (F.col(\"dataset_id\") == dataset_id)\n",
    "            & (F.col(\"effective_start\") <= F.lit(event_ts_lit))\n",
    "            & (\n",
    "                (F.col(\"effective_end\").isNull())\n",
    "                | (F.col(\"effective_end\") > F.lit(event_ts_lit))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # FILTRADO POR AÑO DE FUENTE\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Si existen reglas específicas por año (source_year), se priorizan \n",
    "    # sobre las reglas generales (source_year = NULL).\n",
    "    # ---------------------------------------------------------------------\n",
    "    mappings = mappings.where(\n",
    "        (F.col(\"source_year\").isNull()) | (F.col(\"source_year\") == F.lit(year))\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # RESOLUCIÓN DE CONFLICTOS ENTRE REGLAS\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Si existen múltiples mapeos para el mismo header_source, \n",
    "    # se selecciona el de menor prioridad (columna “priority”).\n",
    "    # ---------------------------------------------------------------------\n",
    "    w = Window.partitionBy(\"header_source\").orderBy(F.col(\"priority\").asc())\n",
    "    mappings_resolved = (\n",
    "        mappings.withColumn(\"rn\", F.row_number().over(w))\n",
    "        .where(F.col(\"rn\") == 1)\n",
    "        .select(\"header_source\", \"header_canonical\")\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # CONSTRUCCIÓN DEL DICCIONARIO DE MAPEOS\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Convierte las reglas resueltas en un diccionario Python \n",
    "    # para fácil aplicación en el DataFrame:\n",
    "    # Ejemplo: {\"MARCA VEHICULO\": \"MARCA\", \"MODELO VEHICULO\": \"MODELO\"}\n",
    "    # ---------------------------------------------------------------------\n",
    "    map_pairs = [(r[\"header_source\"], r[\"header_canonical\"]) for r in mappings_resolved.collect()]\n",
    "    map_dict = dict(map_pairs)\n",
    "\n",
    "    #print(\"mappings:\", map_dict)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # NORMALIZACIÓN DE ENCABEZADOS DEL DATAFRAME\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1. Se eliminan acentos y caracteres especiales.\n",
    "    # 2. Se convierten a mayúsculas.\n",
    "    # 3. Se renombran según el mapeo configurado.\n",
    "    # ---------------------------------------------------------------------\n",
    "    for c in df.columns:\n",
    "        normalizado = (\n",
    "            unicodedata.normalize(\"NFKD\", c)\n",
    "            .encode(\"ASCII\", \"ignore\")\n",
    "            .decode()\n",
    "            .upper()\n",
    "        )\n",
    "        #print(\"norma: \", normalizado)\n",
    "        target = map_dict.get(normalizado)\n",
    "        if target and target != c:\n",
    "            #print(f\"Columna {c} renombrada a {target}\")\n",
    "            df = df.withColumnRenamed(c, target)\n",
    "\n",
    "\n",
    "    #print(\"Columnas Normalizadas:\", df.columns)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # VALIDACIÓN DE COLUMNAS OBLIGATORIAS\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Se asegura que todas las columnas esperadas existan en el DataFrame.\n",
    "    # Si falta alguna, se crea con valor NULL (tipo string).\n",
    "    # ---------------------------------------------------------------------\n",
    "    columnas = [\n",
    "        \"CODIGO_VEHICULO\",\n",
    "        \"FECHA_PROCESO\",\n",
    "        \"FECHA_COMPRA\",\n",
    "        \"CATEGORIA\",\n",
    "        \"TIPO_TRANSACCION\",\n",
    "        \"MARCA\",\n",
    "        \"MODELO\",\n",
    "        \"PAIS\",\n",
    "        \"ANIO_MODELO\",\n",
    "        \"CLASE\",\n",
    "        \"SUBCLASE\",\n",
    "        \"TIPO_VEHICULO\",\n",
    "        \"AVALUO\",\n",
    "        \"TIPO_SERVICIO\",\n",
    "        \"CILINDRAJE\",\n",
    "        \"TIPO_COMBUSTIBLE\",\n",
    "        \"CANTON\",\n",
    "        \"COLOR1\",\n",
    "        \"COLOR2\",\n",
    "        \"TIPO_COMPRADOR\"\n",
    "    ]\n",
    "\n",
    "    for col_name in columnas:\n",
    "        if col_name not in df.columns:\n",
    "            df = df.withColumn(col_name, F.lit(None).cast(\"string\"))\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # REORDENAMIENTO FINAL DE COLUMNAS\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Se fuerza el orden estándar de columnas para garantizar consistencia \n",
    "    # con la estructura esperada en la capa Silver.\n",
    "    # ---------------------------------------------------------------------\n",
    "    df = df.select(*columnas)\n",
    "\n",
    "    #print(\"Columnas Final:\", df.columns)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # RETORNO DEL DATAFRAME NORMALIZADO\n",
    "    # ---------------------------------------------------------------------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203ea34b-8abb-4ec1-b7b4-27f5cc8d30e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Noramlizar los valores de las columnas"
    }
   },
   "outputs": [],
   "source": [
    "def normalizacion_columnas(df, year: int):\n",
    "    \"\"\"\n",
    "    Normaliza y castea columnas de un DataFrame según reglas declarativas\n",
    "    en `tfm_config.column_cast_rules`.\n",
    "\n",
    "    Prioridad de selección de reglas por columna (header_canonical):\n",
    "      1) Reglas con `source_year` específico sobre reglas con `source_year` NULL.\n",
    "      2) Entre reglas empatadas, se elige la de `effective_start` más reciente.\n",
    "\n",
    "    Tipos soportados en `target_type`:\n",
    "      - \"string\" (implícito), \"int\"/\"integer\", \"double\", \"date\", \"timestamp\".\n",
    "\n",
    "    Caso especial para fechas:\n",
    "      - Si `date_format == 'm'`, la columna representa el **mes** y se construye\n",
    "        una fecha `\"01-<mes>-<year>\"` para normalizarla a una fecha válida.\n",
    "    \"\"\"\n",
    "    dataset = \"vehiculos\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) Selección de reglas vigentes para el dataset y el año dado.\n",
    "    #    Se filtran por:\n",
    "    #      - dataset_id\n",
    "    #      - (source_year == year) o (source_year IS NULL)\n",
    "    #      - ventana de vigencia: effective_start <= NOW < effective_end (o end NULL)\n",
    "    # ---------------------------------------------------------------------\n",
    "    ref_ts = F.current_timestamp()\n",
    "    base_rules = (\n",
    "        spark.table(\"tfm_config.column_cast_rules\")\n",
    "        .filter(\n",
    "            (F.col(\"dataset_id\") == F.lit(dataset)) &\n",
    "            ((F.col(\"source_year\").isNull()) | (F.col(\"source_year\") == F.lit(year))) &\n",
    "            (F.col(\"effective_start\").isNull() | (F.col(\"effective_start\") <= ref_ts)) &\n",
    "            (F.col(\"effective_end\").isNull()   | (F.col(\"effective_end\")   >  ref_ts))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) Resolución de conflictos entre reglas de una misma columna.\n",
    "    #    Criterios:\n",
    "    #      a) Preferir reglas con `source_year` (puntaje 1) sobre NULL (0).\n",
    "    #      b) Dentro de cada grupo, elegir la de `effective_start` más reciente.\n",
    "    # ---------------------------------------------------------------------\n",
    "    rules_scored = (\n",
    "        base_rules\n",
    "        .withColumn(\"_has_year\", F.when(F.col(\"source_year\").isNull(), F.lit(0)).otherwise(F.lit(1)))\n",
    "        .withColumn(\"_start_ts\", F.coalesce(F.col(\"effective_start\"), F.lit(\"1970-01-01\").cast(\"timestamp\")))\n",
    "    )\n",
    "\n",
    "    w = Window.partitionBy(\"header_canonical\").orderBy(\n",
    "        F.col(\"_has_year\").desc(),     # primero las que tienen año específico\n",
    "        F.col(\"_start_ts\").desc()      # luego la más reciente por fecha de inicio\n",
    "    )\n",
    "\n",
    "    rules_df = (\n",
    "        rules_scored\n",
    "        .withColumn(\"_rank\", F.row_number().over(w))\n",
    "        .filter(F.col(\"_rank\") == 1)   # nos quedamos con la regla “ganadora”\n",
    "        .drop(\"_has_year\",\"_start_ts\",\"_rank\")\n",
    "    )\n",
    "\n",
    "    # (Opcional) inspección de reglas aplicadas para auditoría/depuración\n",
    "    #display(\n",
    "    #    rules_df.select(\n",
    "    #        \"header_canonical\",\"target_type\",\"date_format\",\"normalize_upper\",\n",
    "    #        \"trim_all\",\"source_year\",\"effective_start\",\"effective_end\",\"note\"\n",
    "    #    )\n",
    "    #)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3) Aplicación de reglas columna por columna.\n",
    "    #    Para cada `header_canonical`:\n",
    "    #      - Se crean columnas ausentes como NULL (string) para mantener esquema.\n",
    "    #      - Se aplican normalizaciones de texto (trim/upper) si corresponde.\n",
    "    #      - Se castea al tipo objetivo controlando formatos y valores inválidos.\n",
    "    # ---------------------------------------------------------------------\n",
    "    for r in rules_df.collect():\n",
    "        col_name    = r[\"header_canonical\"]\n",
    "        target_type = r[\"target_type\"]\n",
    "        date_fmt    = r[\"date_format\"]\n",
    "        do_upper    = r[\"normalize_upper\"]\n",
    "        do_trim     = r[\"trim_all\"]\n",
    "\n",
    "        # Punto de partida: la columna destino (si no existe, se crea como string NULL)\n",
    "        if col_name not in df.columns:\n",
    "            df = df.withColumn(col_name, F.lit(None).cast(\"string\"))\n",
    "        expr = F.col(col_name)\n",
    "\n",
    "        # Normalizaciones textuales previas\n",
    "        if do_trim:\n",
    "            expr = F.trim(expr)\n",
    "        if do_upper:\n",
    "            expr = F.upper(expr)\n",
    "\n",
    "        # Casting según tipo destino\n",
    "        if target_type in (\"int\", \"integer\"):\n",
    "            # Valida que la cadena sea numérica positiva; si no, devuelve NULL\n",
    "            expr = F.when(expr.rlike(r\"^[0-9]+$\"), expr.cast(\"int\")).otherwise(F.lit(None).cast(\"int\"))\n",
    "\n",
    "        elif target_type == \"double\":\n",
    "            # Sustituye coma decimal por punto y castea a double\n",
    "            expr = F.regexp_replace(expr, \",\", \".\").cast(\"double\")\n",
    "\n",
    "        elif target_type == \"date\":\n",
    "            # Quita parte horaria si existiera y se queda con la fecha (antes del primer espacio)\n",
    "            base = F.substring_index(expr.cast(\"string\"), \" \", 1)\n",
    "\n",
    "            if date_fmt == 'm':\n",
    "                # Construye \"01-<mm>-<year>\" y castea con el formato explícito\n",
    "                expr = F.concat_ws(\n",
    "                    \"-\",\n",
    "                    F.lit(\"01\"),\n",
    "                    F.lpad(base, 2, \"0\"),\n",
    "                    F.lit(str(year))\n",
    "                )\n",
    "                expr = F.to_date(expr, \"dd-MM-yyyy\")\n",
    "            else:\n",
    "                if(\"-MMM-\" in date_fmt):\n",
    "                    meses = {\n",
    "                        \"Ene\": \"Jan\", \"Feb\": \"Feb\", \"Mar\": \"Mar\", \"Abr\": \"Apr\",\n",
    "                        \"May\": \"May\", \"Jun\": \"Jun\", \"Jul\": \"Jul\", \"Ago\": \"Aug\",\n",
    "                        \"Sept\": \"Sep\", \"Oct\": \"Oct\", \"Nov\": \"Nov\", \"Dic\": \"Dec\"\n",
    "                    }\n",
    "                    for esp, eng in meses.items():\n",
    "                        base = F.regexp_replace(base, f\"(?i){esp}\", eng)\n",
    "                # Usa formato provisto; si no hay, intenta heurísticas comunes\n",
    "                expr = F.when(\n",
    "                    F.lit(date_fmt).isNotNull(), F.to_date(base, date_fmt)\n",
    "                ).otherwise(\n",
    "                    F.coalesce(\n",
    "                        F.to_date(base, \"dd/MM/yyyy\"),\n",
    "                        F.to_date(base, \"yyyy-MM-dd\"),\n",
    "                        F.to_date(base, \"dd-MM-yyyy\"),\n",
    "                        F.to_date(F.to_timestamp(expr, \"dd/MM/yyyy HH:mm\")),\n",
    "                        F.to_date(F.to_timestamp(expr, \"dd/MM/yyyy HH:mm:ss\")),\n",
    "                        F.to_date(F.to_timestamp(expr, \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        elif target_type == \"timestamp\":\n",
    "            # Casteo a timestamp con formato dado o con heurísticas estándar\n",
    "            if date_fmt:\n",
    "                expr = F.to_timestamp(expr, date_fmt)\n",
    "            else:\n",
    "                expr = F.coalesce(\n",
    "                    F.to_timestamp(expr, \"dd/MM/yyyy HH:mm:ss\"),\n",
    "                    F.to_timestamp(expr, \"dd/MM/yyyy HH:mm\"),\n",
    "                    F.to_timestamp(expr, \"yyyy-MM-dd HH:mm:ss\")\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # Por defecto, mantener como string (tras normalizaciones)\n",
    "            expr = F.trim(expr)\n",
    "            expr = expr.cast(\"string\")\n",
    "\n",
    "        # Reemplaza el contenido de la columna en el DataFrame final\n",
    "        df = df.withColumn(col_name, expr)\n",
    "\n",
    "    # Retorna el DataFrame con las columnas normalizadas y casteadas\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aeeba5e-2de9-42dc-89db-8f5c76f037f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Homologar y Catalogos"
    }
   },
   "outputs": [],
   "source": [
    "def homologar_columnas_categoricas(df):\n",
    "    dominios_columnas = [\n",
    "        (\"tipo_transaccion\",    \"TIPO_TRANSACCION\"),\n",
    "        (\"marca\",               \"MARCA\"),\n",
    "        (\"pais\",                \"PAIS\"),\n",
    "        (\"clase\",               \"CLASE\"),\n",
    "        (\"subclase\",            \"SUBCLASE\"),\n",
    "        (\"tipo_vehiculo\",       \"TIPO_VEHICULO\"),\n",
    "        (\"tipo_servicio\",       \"TIPO_SERVICIO\"),\n",
    "        (\"tipo_combustible\",    \"TIPO_COMBUSTIBLE\"),\n",
    "        (\"tipo_comprador\",      \"TIPO_COMPRADOR\"),\n",
    "    ]\n",
    "\n",
    "    # Tabla de homologación actual\n",
    "    hmg = spark.table(\"tfm_silver_hmg.homologacion\")\n",
    "\n",
    "    nuevas_reglas_all = None\n",
    "\n",
    "    for dominio, col in dominios_columnas:\n",
    "        print(f\"Procesando dominio '{dominio}' / columna '{col}'\")\n",
    "\n",
    "        # 1) valores distintos de esa columna en el DF base\n",
    "        distinct_vals = (\n",
    "            df\n",
    "            .select(F.col(col).alias(\"valor_original\"))\n",
    "            .distinct()\n",
    "            .filter(F.col(\"valor_original\").isNotNull())\n",
    "        )\n",
    "\n",
    "        # 2) reglas vigentes para ese dominio/columna\n",
    "        hmg_dom = (\n",
    "            hmg\n",
    "            .filter(\n",
    "                (F.col(\"dominio\") == F.lit(dominio)) &\n",
    "                (F.col(\"columna_origen\") == F.lit(col)) &\n",
    "                (F.col(\"fecha_hasta\").isNull())\n",
    "            )\n",
    "            .select(\"valor_original\")\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        # 3) valores que NO están aún en la homologadora (left_anti = anti join)\n",
    "        nuevos_valores = distinct_vals.join(hmg_dom, on=\"valor_original\", how=\"left_anti\")\n",
    "\n",
    "        #if nuevos_valores.count() == 0:\n",
    "        #    print(f\"  - No hay nuevos valores para {dominio}/{col}\")\n",
    "        #    continue\n",
    "\n",
    "        # 4) crear filas para homologacion\n",
    "        nuevas_reglas = (\n",
    "            nuevos_valores\n",
    "            .withColumn(\"dominio\", F.lit(dominio))\n",
    "            .withColumn(\"columna_origen\", F.lit(col))\n",
    "            .withColumn(\"valor_original\", F.col(\"valor_original\"))\n",
    "            .withColumn(\"catalog_id\", F.lit(None).cast(\"bigint\"))\n",
    "            .withColumn(\"fecha_desde\", F.current_timestamp())\n",
    "            .withColumn(\"fecha_hasta\", F.lit(None).cast(\"timestamp\"))\n",
    "            .select(\n",
    "                \"dominio\",\n",
    "                \"columna_origen\",\n",
    "                \"valor_original\",\n",
    "                \"catalog_id\",\n",
    "                \"fecha_desde\",\n",
    "                \"fecha_hasta\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if nuevas_reglas_all is None:\n",
    "            nuevas_reglas_all = nuevas_reglas\n",
    "        else:\n",
    "            nuevas_reglas_all = nuevas_reglas_all.unionByName(nuevas_reglas)\n",
    "\n",
    "    # 5) Insertar todo de una sola vez\n",
    "    if nuevas_reglas_all is not None and  nuevas_reglas_all.count() > 0:\n",
    "        print(\"Insertando nuevas reglas en tfm_silver_hmg.homologacion...\")\n",
    "        (\n",
    "            nuevas_reglas_all\n",
    "            .write\n",
    "            .mode(\"append\")\n",
    "            .format(\"delta\")\n",
    "            .saveAsTable(\"tfm_silver_hmg.homologacion\")\n",
    "        )\n",
    "    else:\n",
    "        print(\"No hay nuevas reglas que insertar.\")\n",
    "\n",
    "    df.createOrReplaceTempView(\"vehiculos_sin_hmg_ctg\")\n",
    "\n",
    "    df_con_ids = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        v.*,\n",
    "        \n",
    "        --TIPO_TRANSACCION\n",
    "        h_tt.regla_id                               AS tipo_transaccion_hmg_id,\n",
    "        NVL(h_tt.catalog_id, 1)                     AS tipo_transaccion_id,\n",
    "\n",
    "        -- MARCA\n",
    "        h_marca.regla_id                            AS marca_hmg_id,\n",
    "        NVL(h_marca.catalog_id, 1)                 AS marca_id,\n",
    "\n",
    "        -- PAIS\n",
    "        h_pais.regla_id                             AS pais_hmg_id,\n",
    "        NVL(h_pais.catalog_id,1)                    AS pais_id,\n",
    "\n",
    "        -- CLASE\n",
    "        h_clase.regla_id                            AS clase_hmg_id,\n",
    "        NVL(h_clase.catalog_id,1)                   AS clase_id,\n",
    "\n",
    "        -- SUBCLASE\n",
    "        h_subclase.regla_id                         AS subclase_hmg_id,\n",
    "        NVL(h_subclase.catalog_id,1)                AS subclase_id,\n",
    "\n",
    "        -- TIPO_VEHICULO\n",
    "        h_tipo_veh.regla_id                         AS tipo_vehiculo_hmg_id,\n",
    "        NVL(h_tipo_veh.catalog_id,1)                AS tipo_vehiculo_id,\n",
    "\n",
    "        -- TIPO_SERVICIO\n",
    "        h_tipo_srv.regla_id                         AS tipo_servicio_hmg_id,\n",
    "        NVL(h_tipo_srv.catalog_id,1)                AS tipo_servicio_id,\n",
    "\n",
    "        -- TIPO_COMBUSTIBLE\n",
    "        h_comb.regla_id                             AS tipo_combustible_hmg_id,\n",
    "        NVL(h_comb.catalog_id,1)                    AS tipo_combustible_id,\n",
    "\n",
    "        -- TIPO_COMPRADOR\n",
    "        h_comp.regla_id                             AS tipo_comprador_hmg_id,\n",
    "        NVL(h_comp.catalog_id,1)                    AS tipo_comprador_id\n",
    "\n",
    "    FROM vehiculos_sin_hmg_ctg v\n",
    "    -- ================= TIPO TRANSCCION =================\n",
    "    LEFT JOIN tfm_silver_hmg.homologacion h_tt\n",
    "        ON h_tt.dominio        = 'tipo_transaccion'\n",
    "    AND h_tt.columna_origen = 'TIPO_TRANSACCION'\n",
    "    AND h_tt.valor_original = v.TIPO_TRANSACCION\n",
    "    AND h_tt.fecha_hasta IS NULL         -- regla vigente\n",
    "\n",
    "    -- ================= MARCA =================\n",
    "    LEFT JOIN tfm_silver_hmg.homologacion h_marca\n",
    "        ON h_marca.dominio        = 'marca'\n",
    "    AND h_marca.columna_origen = 'MARCA'\n",
    "    AND h_marca.valor_original = v.MARCA\n",
    "    AND h_marca.fecha_hasta IS NULL         -- regla vigente\n",
    "\n",
    "\n",
    "    -- ================= PAIS =================\n",
    "    LEFT JOIN tfm_silver_hmg.homologacion h_pais\n",
    "        ON h_pais.dominio        = 'pais'\n",
    "    AND h_pais.columna_origen = 'PAIS'\n",
    "    AND h_pais.valor_original = v.PAIS\n",
    "    AND h_pais.fecha_hasta IS NULL\n",
    "    \n",
    "\n",
    "    -- ================= CLASE =================\n",
    "    LEFT JOIN tfm_silver_hmg.homologacion h_clase\n",
    "        ON h_clase.dominio        = 'clase'\n",
    "    AND h_clase.columna_origen = 'CLASE'\n",
    "    And h_clase.valor_original = v.CLASE\n",
    "    AND h_clase.fecha_hasta IS NULL\n",
    "\n",
    "    -- ================= SUBCLASE =================\n",
    "    LEFT JOIN tfm_silver_hmg.homologacion h_subclase\n",
    "        ON h_subclase.dominio        = 'subclase'\n",
    "    AND h_subclase.columna_origen = 'SUBCLASE'\n",
    "    AND h_subclase.valor_original = v.SUBCLASE\n",
    "    AND h_subclase.fecha_hasta IS NULL\n",
    "\n",
    "    -- ================= TIPO_VEHICULO =================\n",
    "    LEFT JOIN tfm_silver_hmg.homologacion h_tipo_veh\n",
    "        ON h_tipo_veh.dominio        = 'tipo_vehiculo'\n",
    "    AND h_tipo_veh.columna_origen = 'TIPO_VEHICULO'\n",
    "    AND h_tipo_veh.valor_original = v.TIPO_VEHICULO\n",
    "    AND h_tipo_veh.fecha_hasta IS NULL\n",
    "\n",
    "    -- ================= TIPO_SERVICIO =================\n",
    "    LEFT JOIN tfm_silver_hmg.homologacion h_tipo_srv\n",
    "        ON h_tipo_srv.dominio        = 'tipo_servicio'\n",
    "    AND h_tipo_srv.columna_origen = 'TIPO_SERVICIO'\n",
    "    AND h_tipo_srv.valor_original = v.TIPO_SERVICIO\n",
    "    AND h_tipo_srv.fecha_hasta IS NULL\n",
    "\n",
    "    -- ================= TIPO_COMBUSTIBLE =================\n",
    "    LEFT JOIN tfm_silver_hmg.homologacion h_comb\n",
    "        ON h_comb.dominio        = 'tipo_combustible'\n",
    "    AND h_comb.columna_origen = 'TIPO_COMBUSTIBLE'\n",
    "    AND h_comb.valor_original = v.TIPO_COMBUSTIBLE\n",
    "    AND h_comb.fecha_hasta IS NULL\n",
    "\n",
    "    -- ================= TIPO_COMPRADOR =================\n",
    "    LEFT JOIN tfm_silver_hmg.homologacion h_comp\n",
    "        ON h_comp.dominio        = 'tipo_comprador'\n",
    "    AND h_comp.columna_origen = 'TIPO_COMPRADOR'\n",
    "    AND h_comp.valor_original = v.TIPO_COMPRADOR\n",
    "    AND h_comp.fecha_hasta IS NULL\n",
    "    ;\n",
    "    \"\"\")\n",
    "\n",
    "    return df_con_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa11a687-5e8a-4413-9fc1-45b7d61dcb75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preparar para la insercion en silver"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_silver(df_in,\n",
    "                       source_year: int,\n",
    "                       source_file: str,\n",
    "                       source_sha256: str = None):\n",
    "    \"\"\"\n",
    "    Prepara un DataFrame proveniente de la capa Bronze para su escritura o \n",
    "    integración (MERGE) en la capa Silver.\n",
    "\n",
    "    Esta función agrega columnas técnicas, metadatos de linaje, \n",
    "    y hashes de negocio y contenido para detectar cambios y evitar duplicados.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    df_in : pyspark.sql.DataFrame\n",
    "        DataFrame de entrada ya normalizado (con FECHA_PROCESO, FECHA_COMPRA y CODIGO_VEHICULO).\n",
    "    source_year : int\n",
    "        Año de origen del archivo procesado (derivado de la carpeta o metadata del archivo S3).\n",
    "    source_file : str\n",
    "        Nombre completo del archivo fuente (por trazabilidad).\n",
    "    source_sha256 : str, opcional\n",
    "        Hash del archivo fuente (para control de integridad de datos).\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        DataFrame enriquecido con columnas de control, linaje, \n",
    "        y claves hash para manejo incremental en la capa Silver.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) Agregar columnas técnicas y de linaje\n",
    "    # ---------------------------------------------------------------------\n",
    "    # - anio_proceso / mes_proceso: derivadas de FECHA_PROCESO para \n",
    "    #   facilitar particionamiento y consultas temporales.\n",
    "    # - source_year / source_file / source_sha256: trazabilidad del origen.\n",
    "    # - ingest_ts: timestamp del momento en que se ingesta el archivo.\n",
    "    # ---------------------------------------------------------------------\n",
    "    df = (\n",
    "        df_in\n",
    "        .withColumn(\"anio_proceso\", F.year(\"FECHA_PROCESO\"))\n",
    "        .withColumn(\"mes_proceso\",  F.month(\"FECHA_PROCESO\"))\n",
    "        .withColumn(\"source_year\",  F.lit(int(source_year)))\n",
    "        .withColumn(\"source_file\",  F.lit(source_file))\n",
    "        .withColumn(\"source_sha256\", F.lit(source_sha256))\n",
    "        .withColumn(\"ingest_ts\",    F.current_timestamp())\n",
    "        # NOTA: si necesitás convertir tipos explícitamente, \n",
    "        # podés habilitar las siguientes líneas:\n",
    "        # .withColumn(\"ANIO_MODELO\", F.col(\"ANIO_MODELO\").cast(IntegerType()))\n",
    "        # .withColumn(\"CILINDRAJE\",  F.col(\"CILINDRAJE\").cast(IntegerType()))\n",
    "        # .withColumn(\"AVALUO\",      F.regexp_replace(F.col(\"AVALUO\"), \",\", \".\").cast(DoubleType()))\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) Generar clave de negocio (Business Key)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Define los campos que identifican de forma única una transacción \n",
    "    # o registro de vehículo. En este caso:\n",
    "    #   (CODIGO_VEHICULO, FECHA_COMPRA)\n",
    "    #   (Se excluye FECHA_PROCESO porque el mismo vehículo puede \n",
    "    #   volver a aparecer con diferentes fechas de proceso)\n",
    "    #\n",
    "    # Se genera un hash SHA-256 concatenando los valores (como strings),\n",
    "    # usando \"||\" como separador para evitar ambigüedades.\n",
    "    # ---------------------------------------------------------------------\n",
    "    bk_cols = [\"CODIGO_VEHICULO\", \"FECHA_COMPRA\"]\n",
    "    df = df.withColumn(\n",
    "        \"bk_hash\",\n",
    "        F.sha2(\n",
    "            F.concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in bk_cols]),\n",
    "            256\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3) Generar hash de contenido (Content Hash)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Permite detectar si un registro cambió en alguno de sus valores \n",
    "    # no clave (por ejemplo, cambio en marca, modelo, avaluo, etc.)\n",
    "    # \n",
    "    # Se concatena el contenido completo de las columnas relevantes, \n",
    "    # aplicando un hash SHA-256 que representa el “estado” del registro.\n",
    "    # ---------------------------------------------------------------------\n",
    "    content_cols = [\n",
    "        \"CATEGORIA\",\n",
    "        #\"TIPO_TRANSACCION\",\n",
    "        \"tipo_transaccion_hmg_id\",\n",
    "        #\"MARCA\",\n",
    "        \"marca_hmg_id\",\n",
    "        \"MODELO\",\n",
    "        #\"PAIS\",\n",
    "        \"pais_hmg_id\",\n",
    "        \"ANIO_MODELO\",\n",
    "        #\"CLASE\",\n",
    "        \"clase_hmg_id\",\n",
    "        #\"SUBCLASE\",\n",
    "        \"subclase_hmg_id\",\n",
    "        #\"TIPO_VEHICULO\",\n",
    "        \"tipo_vehiculo_hmg_id\",\n",
    "        \"AVALUO\",\n",
    "        #\"TIPO_SERVICIO\",\n",
    "        \"tipo_servicio_hmg_id\",\n",
    "        \"CILINDRAJE\",\n",
    "        #\"TIPO_COMBUSTIBLE\",\n",
    "        \"tipo_combustible_hmg_id\",\n",
    "        \"CANTON\",\n",
    "        \"COLOR1\",\n",
    "        \"COLOR2\",\n",
    "        #\"TIPO_COMPRADOR\"\n",
    "        \"tipo_comprador_hmg_id\"\n",
    "    ]\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"content_hash\",\n",
    "        F.sha2(\n",
    "            F.concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in content_cols]),\n",
    "            256\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4) Retornar DataFrame enriquecido\n",
    "    # ---------------------------------------------------------------------\n",
    "    # El resultado contiene todas las columnas de negocio más:\n",
    "    #   - anio_proceso, mes_proceso\n",
    "    #   - source_year, source_file, source_sha256\n",
    "    #   - ingest_ts, bk_hash, content_hash\n",
    "    #\n",
    "    # Este DataFrame está listo para ser:\n",
    "    #   - Insertado en la tabla Silver.\n",
    "    #   - Usado en un MERGE (upsert) incremental.\n",
    "    # ---------------------------------------------------------------------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27f6f68-99bc-45ba-9083-66da4caee2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def control_duplicados(df):\n",
    "    \"\"\"\n",
    "    Elimina registros duplicados en un DataFrame antes de su inserción en la capa Silver.\n",
    "    Optimizada para ejecución distribuida en PySpark.\n",
    "\n",
    "    Reglas:\n",
    "      1. Elimina duplicados exactos basados en (bk_hash, content_hash, FECHA_PROCESO).\n",
    "      2. Si existen varias versiones con la misma clave (bk_hash + content_hash),\n",
    "         conserva únicamente la más antigua (FECHA_PROCESO más baja).\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) Eliminación directa de duplicados exactos\n",
    "    # ---------------------------------------------------------------------\n",
    "    # dropDuplicates() ya es distribuido y no requiere un shuffle completo.\n",
    "    df = df.dropDuplicates([\"bk_hash\", \"content_hash\", \"FECHA_PROCESO\"])\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) Selección de la versión más antigua por hash\n",
    "    # ---------------------------------------------------------------------\n",
    "    # En lugar de usar una ventana + row_number (que genera un shuffle caro),\n",
    "    # podemos usar un groupBy + agg(min()) que es más eficiente y escalable.\n",
    "    # Esto logra el mismo efecto: conservar la fila con la FECHA_PROCESO mínima.\n",
    "    # ---------------------------------------------------------------------\n",
    "    df = df.repartition(\"bk_hash\")  # agrupa por hash y reduce shuffle posterior\n",
    "    min_dates = (\n",
    "        df.groupBy(\"bk_hash\", \"content_hash\")\n",
    "          .agg(F.min(\"FECHA_PROCESO\").alias(\"min_fecha_proceso\"))\n",
    "    )\n",
    "\n",
    "    # Join distribuido (sin broadcast, Spark manejará la estrategia óptima)\n",
    "    df = (\n",
    "        df.join(\n",
    "            min_dates,\n",
    "            on=[\n",
    "                df[\"bk_hash\"] == min_dates[\"bk_hash\"],\n",
    "                df[\"content_hash\"] == min_dates[\"content_hash\"],\n",
    "                df[\"FECHA_PROCESO\"] == min_dates[\"min_fecha_proceso\"],\n",
    "            ],\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .select(df[\"*\"])  # mantener el esquema original\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c67ffe1d-8c03-493e-9524-683be1a660d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Insertar registros"
    }
   },
   "outputs": [],
   "source": [
    "def insertar_datos(df, bandera_insertar: bool):\n",
    "    \"\"\"\n",
    "    Inserta los registros nuevos o actualizados desde un DataFrame temporal (vehiculos_stage)\n",
    "    en la tabla Delta principal tfm_silver.vehiculos. \n",
    "\n",
    "    El proceso incluye los siguientes pasos:\n",
    "    1. Determinar los años (particiones) presentes en la carga actual.\n",
    "    2. Extraer únicamente las particiones correspondientes desde la tabla Silver.\n",
    "    3. Identificar los registros nuevos o modificados mediante un anti-join.\n",
    "    4. Insertar los registros resultantes en la tabla Silver.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame con los registros listos para inserción en Silver.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Configuración de base de datos y tabla destino\n",
    "    # -------------------------------------------------------------------------\n",
    "    db_name    = \"tfm_silver\"\n",
    "    silver_tbl = f\"{db_name}.vehiculos\"\n",
    "\n",
    "    # Selecciona el esquema de trabajo\n",
    "    spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "    # Registra el DataFrame recibido como vista temporal\n",
    "    df.createOrReplaceTempView(\"vehiculos_stage\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. Determinación de los años (particiones) presentes en la carga actual\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Obtiene la lista de años distintos desde el DataFrame de staging\n",
    "    target_years = [\n",
    "        r['anio_proceso']\n",
    "        for r in spark.table(\"vehiculos_stage\")\n",
    "                      .select(\"anio_proceso\")\n",
    "                      .distinct()\n",
    "                      .collect()\n",
    "    ]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. Extracción de las particiones relevantes desde la tabla Silver\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Carga únicamente las filas correspondientes a los años identificados.\n",
    "    # Esta estrategia evita escanear la totalidad de la tabla Silver.\n",
    "    silver_slice = (\n",
    "        spark.table(silver_tbl)\n",
    "        .where(F.col(\"anio_proceso\").isin(target_years))\n",
    "        .select(\n",
    "            \"anio_proceso\",\n",
    "            \"CODIGO_VEHICULO\",\n",
    "            \"FECHA_PROCESO\",\n",
    "            \"FECHA_COMPRA\",\n",
    "            \"content_hash\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Carga de la vista temporal como DataFrame de staging\n",
    "    stage = spark.table(\"vehiculos_stage\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. Anti-join para identificar registros nuevos o modificados\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Se comparan las filas de staging con la porción relevante de Silver\n",
    "    # utilizando la clave compuesta y el hash de contenido.\n",
    "    # Se retienen únicamente los registros que no existen en Silver.\n",
    "    to_insert = (\n",
    "        stage.alias(\"s\")\n",
    "        .join(\n",
    "            silver_slice.alias(\"t\"),\n",
    "            on=[\n",
    "                F.col(\"t.anio_proceso\")    == F.col(\"s.anio_proceso\"),\n",
    "                F.col(\"t.CODIGO_VEHICULO\") == F.col(\"s.CODIGO_VEHICULO\"),\n",
    "                F.col(\"t.FECHA_PROCESO\")   == F.col(\"s.FECHA_PROCESO\"),\n",
    "                F.col(\"t.FECHA_COMPRA\")    == F.col(\"s.FECHA_COMPRA\"),\n",
    "                F.col(\"t.content_hash\")    == F.col(\"s.content_hash\")\n",
    "            ],\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        # Reparticiona los datos por año de proceso para mejorar la escritura\n",
    "        .repartition(\"anio_proceso\")\n",
    "    )\n",
    "\n",
    "    schema_cols = spark.table(silver_tbl).columns\n",
    "    to_insert = to_insert.select(*schema_cols)\n",
    "    \n",
    "    #to_insert = to_insert.persist()\n",
    "    numero = to_insert.count()\n",
    "    #print(\"Numero registros nuevos:\", numero)\n",
    "    #numero = 99999999\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. Inserción en la tabla Delta particionada (Silver)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Los registros resultantes del anti-join se escriben en la tabla Delta.\n",
    "    # Se utiliza modo append dado que la tabla ya está particionada por anio_proceso.\n",
    "    if bandera_insertar == True:\n",
    "        (\n",
    "            to_insert.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(silver_tbl)\n",
    "        )\n",
    "\n",
    "    return numero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea4e886-8cf4-4f8b-816d-314aaf7fd9a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PROCESO LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9464b01-0a88-497f-9821-dda3dd4c2bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar los valores de cada columna de to_process en variables y mostrarlos\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "if to_process.count() == 0:\n",
    "    print(\"No hay registros por procesar\")\n",
    "\n",
    "bandera_insertar = True\n",
    "    \n",
    "for row in to_process.collect():\n",
    "    year = row[\"year\"]\n",
    "    last_metadata_update = row[\"last_metadata_update\"]\n",
    "    last_status = row[\"last_status\"]\n",
    "    last_checked_ts = row[\"last_checked_ts\"]\n",
    "    last_file_path = row[\"last_file_path\"]\n",
    "    last_sha256 = row[\"last_sha256\"]\n",
    "    last_sha256_silver = row[\"last_sha256_silver\"]\n",
    "\n",
    "    ruta_archivo = 'raw/vehiculos/'\n",
    "    file_name = os.path.basename(last_file_path)\n",
    "\n",
    "    print(f\"\\n============   {year}  ============\")\n",
    "    print(f\"Inicio Archivo: {file_name}\")\n",
    "    print(f\"Fecha Proceso: {last_metadata_update}\")\n",
    "    print(f\"Estado: {last_status}\")\n",
    "    print(f\"Fecha Proceso: {last_checked_ts}\")\n",
    "    print(f\"Archivo: {last_file_path}\")\n",
    "    print(f\"Hash: {last_sha256}\")\n",
    "\n",
    "    print(\"\\n--------------\")\n",
    "    print(\"Obtener df de csv\")\n",
    "    inicio = time.time()\n",
    "    df_csv = None\n",
    "    df_csv = obtener_dataframe_csv(f\"{PATH_VOLUMEN}/{ruta_archivo}year={year}/{file_name}\")\n",
    "    #display(df_csv.limit(5))\n",
    "    fin = time.time()\n",
    "    duracion = fin - inicio\n",
    "    print(f\"[INFO] Tiempo de ejecución de Obtener df de csv: {duracion:.2f} segundos\")\n",
    "\n",
    "    print(\"\\n--------------\")\n",
    "    print(\"Normalizar cabeceras\")\n",
    "    inicio = time.time()\n",
    "    df_cabeceras = None\n",
    "    df_cabeceras = normalizacion_cabecera(df_csv, year)\n",
    "    #display(df_cabeceras.limit(5))\n",
    "    fin = time.time()\n",
    "    duracion = fin - inicio\n",
    "    print(f\"[INFO] Tiempo de ejecución de Normalizar cabeceras: {duracion:.2f} segundos\")\n",
    "\n",
    "    print(\"\\n--------------\")\n",
    "    print(\"Normalizar columnas\")\n",
    "    inicio = time.time()\n",
    "    df_columnas_normalizadas = None\n",
    "    df_columnas_normalizadas = normalizacion_columnas(df_cabeceras, year)\n",
    "    #display(df_columnas_normalizadas.limit(5))\n",
    "    fin = time.time()\n",
    "    duracion = fin - inicio\n",
    "    print(f\"[INFO] Tiempo de ejecución de Normalizar columnas: {duracion:.2f} segundos\")\n",
    "\n",
    "    print(\"\\n--------------\")\n",
    "    print(\"Homologar y Categorizar columnas\")\n",
    "    inicio = time.time()\n",
    "    df_columnas_homologadas_catalogos = None\n",
    "    df_columnas_homologadas_catalogos= homologar_columnas_categoricas(df_columnas_normalizadas)\n",
    "    #display(df_columnas_homologadas_catalogos.limit(5))\n",
    "    fin = time.time()\n",
    "    duracion = fin - inicio\n",
    "    print(f\"[INFO] Tiempo de ejecución de Homologar y catalogos columnas: {duracion:.2f} segundos\")\n",
    "\n",
    "    print(\"\\n--------------\")\n",
    "    print(\"Agregar Columnas\")\n",
    "    inicio = time.time()\n",
    "    df_silver = None\n",
    "    df_silver = prepare_for_silver(df_columnas_homologadas_catalogos, year, last_file_path, last_sha256)\n",
    "    #display(df_silver.limit(5))\n",
    "    fin = time.time()\n",
    "    duracion = fin - inicio\n",
    "    print(f\"[INFO] Tiempo de ejecución de Agregar Columnas: {duracion:.2f} segundos\")\n",
    "\n",
    "    print(\"\\n--------------\")\n",
    "    print(\"Control duplicidad\")\n",
    "    inicio = time.time()\n",
    "    #print(\"Conteo Original: \", df_silver.count())\n",
    "    df_sin_dupplicados = control_duplicados(df_silver)\n",
    "    #print(\"Conteo Sin duplicados: \", df_sin_dupplicados.count())\n",
    "    fin = time.time()\n",
    "    duracion = fin - inicio\n",
    "    print(f\"[INFO] Tiempo de ejecución de Control duplicidad: {duracion:.2f} segundos\")\n",
    "\n",
    "    print(\"\\n--------------\")\n",
    "    print(\"Insertar Registros\")\n",
    "    inicio = time.time()\n",
    "    numero_nuevos_registros = insertar_datos(df_sin_dupplicados, bandera_insertar)\n",
    "    fin = time.time()\n",
    "    duracion = fin - inicio\n",
    "    print(f\"[INFO] Tiempo de ejecución de Insertar Registro: {duracion:.2f} segundos\")\n",
    "\n",
    "    if (numero_nuevos_registros > 0) and bandera_insertar:\n",
    "        print(f\"Se insertaron {numero_nuevos_registros} nuevos registros\")\n",
    "        print(\"\\n--------------\")\n",
    "        print(\"Insertar state_files_processed Silver\")\n",
    "\n",
    "        process_ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        spark.sql(f\"\"\"\n",
    "            DELETE FROM tfm_silver.state_files_processed\n",
    "            WHERE last_sha256 = '{last_sha256_silver}'\n",
    "            AND year = {year};\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO tfm_silver.state_files_processed VALUES (\n",
    "                {year},\n",
    "                '{last_metadata_update}',\n",
    "                '{last_status}',\n",
    "                '{last_checked_ts}',\n",
    "                '{last_file_path}',\n",
    "                '{last_sha256}',\n",
    "                '{process_ts}',\n",
    "                NULL\n",
    "            );\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(\"No se insertaron nuevos registros\")\n",
    "        print(\"\\n--------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9074b37a-5d1d-41f3-af1e-086f07635256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pruebas Manuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581d36c9-34f2-43f4-b87f-46347bf623d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import time\n",
    "\n",
    "bandera_insertar = True\n",
    "    \n",
    "row =  to_process.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7581559-d5dc-47dd-a558-3f4a75cf42ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "year = row[\"year\"]\n",
    "last_metadata_update = row[\"last_metadata_update\"]\n",
    "last_status = row[\"last_status\"]\n",
    "last_checked_ts = row[\"last_checked_ts\"]\n",
    "last_file_path = row[\"last_file_path\"]\n",
    "last_sha256 = row[\"last_sha256\"]\n",
    "last_sha256_silver = row[\"last_sha256_silver\"]\n",
    "\n",
    "ruta_archivo = 'raw/vehiculos/'\n",
    "file_name = os.path.basename(last_file_path)\n",
    "\n",
    "print(f\"\\n============   {year}  ============\")\n",
    "print(f\"Inicio Archivo: {file_name}\")\n",
    "print(f\"Fecha Proceso: {last_metadata_update}\")\n",
    "print(f\"Estado: {last_status}\")\n",
    "print(f\"Fecha Proceso: {last_checked_ts}\")\n",
    "print(f\"Archivo: {last_file_path}\")\n",
    "print(f\"Hash: {last_sha256}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef5b662-c16d-492b-be9b-306b37cc0875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"\\n--------------\")\n",
    "print(\"Obtener df de csv\")\n",
    "inicio = time.time()\n",
    "df_csv = None\n",
    "df_csv = obtener_dataframe_csv(f\"{PATH_VOLUMEN}/{ruta_archivo}year={year}/{file_name}\")\n",
    "display(df_csv.limit(5))\n",
    "fin = time.time()\n",
    "duracion = fin - inicio\n",
    "print(f\"[INFO] Tiempo de ejecución de Obtener df de csv: {duracion:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2855ca-5255-4ac2-940a-bf5ff1610f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"\\n--------------\")\n",
    "print(\"Normalizar cabeceras\")\n",
    "inicio = time.time()\n",
    "df_cabeceras = None\n",
    "df_cabeceras = normalizacion_cabecera(df_csv, year)\n",
    "display(df_cabeceras.limit(5))\n",
    "fin = time.time()\n",
    "duracion = fin - inicio\n",
    "print(f\"[INFO] Tiempo de ejecución de Normalizar cabeceras: {duracion:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23e4029-a150-432a-9c54-4823fd69c25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"\\n--------------\")\n",
    "print(\"Normalizar columnas\")\n",
    "inicio = time.time()\n",
    "df_columnas_normalizadas = None\n",
    "df_columnas_normalizadas = normalizacion_columnas(df_cabeceras, year)\n",
    "display(df_columnas_normalizadas.limit(5))\n",
    "fin = time.time()\n",
    "duracion = fin - inicio\n",
    "print(f\"[INFO] Tiempo de ejecución de Normalizar columnas: {duracion:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdec6cf-35f0-4b0f-9cf2-694e51a7d7a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"\\n--------------\")\n",
    "print(\"Homologar y Categorizar columnas\")\n",
    "inicio = time.time()\n",
    "df_columnas_homologadas_catalogos = None\n",
    "df_columnas_homologadas_catalogos= homologar_columnas_categoricas(df_columnas_normalizadas)\n",
    "display(df_columnas_homologadas_catalogos.limit(5))\n",
    "fin = time.time()\n",
    "duracion = fin - inicio\n",
    "print(f\"[INFO] Tiempo de ejecución de Homologar y catalogos columnas: {duracion:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a48298c-768f-42cc-8d1d-e09c32ad257c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"\\n--------------\")\n",
    "print(\"Agregar Columnas\")\n",
    "inicio = time.time()\n",
    "df_silver = None\n",
    "df_silver = prepare_for_silver(df_columnas_homologadas_catalogos, year, last_file_path, last_sha256)\n",
    "display(df_silver.limit(5))\n",
    "fin = time.time()\n",
    "duracion = fin - inicio\n",
    "print(f\"[INFO] Tiempo de ejecución de Agregar Columnas: {duracion:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb865318-8c00-4433-8e75-2667d575683f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"\\n--------------\")\n",
    "print(\"Control duplicidad\")\n",
    "inicio = time.time()\n",
    "print(\"Conteo Original: \", df_silver.count())\n",
    "df_sin_dupplicados = control_duplicados(df_silver)\n",
    "print(\"Conteo Sin duplicados: \", df_sin_dupplicados.count())\n",
    "fin = time.time()\n",
    "duracion = fin - inicio\n",
    "print(f\"[INFO] Tiempo de ejecución de Control duplicidad: {duracion:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac30ee94-08e3-4eba-95a7-119919f6869c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"\\n--------------\")\n",
    "print(\"Insertar Registros\")\n",
    "inicio = time.time()\n",
    "numero_nuevos_registros = insertar_datos(df_sin_dupplicados, True)\n",
    "fin = time.time()\n",
    "duracion = fin - inicio\n",
    "print(f\"[INFO] Tiempo de ejecución de Insertar Registro: {duracion:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "337cc5de-6e28-47c5-bb20-ae3d00f5ade8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "if (numero_nuevos_registros > 0) and bandera_insertar:\n",
    "    print(f\"Se insertaron {numero_nuevos_registros} nuevos registros\")\n",
    "    print(\"\\n--------------\")\n",
    "    print(\"Insertar state_files_processed Silver\")\n",
    "\n",
    "    process_ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    spark.sql(f\"\"\"\n",
    "        DELETE FROM tfm_silver.state_files_processed\n",
    "        WHERE last_sha256 = '{last_sha256_silver}'\n",
    "        AND year = {year};\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO tfm_silver.state_files_processed VALUES (\n",
    "            {year},\n",
    "            '{last_metadata_update}',\n",
    "            '{last_status}',\n",
    "            '{last_checked_ts}',\n",
    "            '{last_file_path}',\n",
    "            '{last_sha256}',\n",
    "            '{process_ts}',\n",
    "            NULL\n",
    "        );\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"No se insertaron nuevos registros\")\n",
    "    print(\"\\n--------------\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5601542025674972,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_carga_datos_vehiculos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
